{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the deployment\n",
    "## Using gRPC for communicating with TF Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(\"../scripts/utils.py\"))\n",
    "\n",
    "import grpc\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "import cloudpickle\n",
    "from utils import init_configure, preprocessing_text, preprocessing_keyword, get_tokens\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def np_to_protobuf(data):\n",
    "    return tf.make_tensor_proto(data, shape=data.shape)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher, _, max_length_tweet, max_length_keyword = init_configure(nlp)\n",
    "\n",
    "with open('../models/tokenizer.bin', 'rb') as f_in:\n",
    "    tokenizer, dict_words = cloudpickle.load(f_in)\n",
    "    \n",
    "host = os.getenv('TF_SERVING_HOST', 'localhost:8500')\n",
    "channel = grpc.insecure_channel(host, options=(('grpc.enable_http_proxy', 0),))\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Disaster Tweet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#docker run -it --rm -p 8500:8500 -v \"$(pwd)/scripts/disaster_tweets_model:/models/disaster_tweets_model/1\" -e MODEL_NAME=disaster_tweets_model tensorflow/serving:2.14.0\n",
    "\n",
    "tweet = {\"keyword\":\"ablaze\",\n",
    "         \"location\":\"London\",\n",
    "         \"text\": \"Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham's Wholesale Market http://t.co/irWqCEZWEU\"}\n",
    "\n",
    "clean_text, n_words, n_characters, n_hashtags, n_mentions, n_urls, n_punctuations = preprocessing_text(tweet[\"text\"], nlp, matcher, dict_words)\n",
    "clean_keyword = preprocessing_keyword(tweet[\"keyword\"], nlp)\n",
    "\n",
    "tokenized_text = get_tokens([clean_text], tokenizer=tokenizer, max_length=max_length_tweet, fit=False, padding=True)\n",
    "tokenized_keyword = get_tokens([clean_keyword], tokenizer=tokenizer, max_length=max_length_keyword, fit=False, padding=True)\n",
    "context = np.array([n_words, n_characters, n_hashtags, n_mentions, n_urls, n_punctuations])\n",
    "\n",
    "tokenized_text = tokenized_text.astype(np.float32)\n",
    "tokenized_keyword = tokenized_keyword.astype(np.float32)\n",
    "context = np.expand_dims(context, axis=0).astype(np.float32)\n",
    "\n",
    "pb_request = predict_pb2.PredictRequest()\n",
    "pb_request.model_spec.name = 'disaster_tweets_model'\n",
    "pb_request.model_spec.signature_name = 'serving_default'\n",
    "\n",
    "pb_request.inputs['text'].CopyFrom(np_to_protobuf(tokenized_text))\n",
    "pb_request.inputs['keyword'].CopyFrom(np_to_protobuf(tokenized_keyword))\n",
    "pb_request.inputs['context'].CopyFrom(np_to_protobuf(context))\n",
    "pb_result = stub.Predict(pb_request, timeout=40.0)\n",
    "pred = pb_result.outputs['dense_5'].float_val\n",
    "[\"Not Disaster Tweet\", \"Disaster Tweet\"][round(pred[0])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlzoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
